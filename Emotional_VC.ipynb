{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the following Package Versions:\n",
      "---------------------------------------\n",
      "Matplotlib      V.3.7.1\n",
      "NumPy           V.1.23.5\n",
      "Pandas          V.2.0.0\n",
      "Sci-Kit Learn   V.1.2.2\n",
      "Seaborn         V.0.12.2\n",
      "TensorFlow      V.2.13.0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import html5lib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import shifterator as sh\n",
    "import tensorflow as tf\n",
    "import sklearn as skl\n",
    "\n",
    "print('Running the following Package Versions:')\n",
    "print('---------------------------------------')\n",
    "\n",
    "print(f'Matplotlib      V.{mpl.__version__}')\n",
    "print(f'NumPy           V.{np.__version__}')\n",
    "print(f'Pandas          V.{pd.__version__}')\n",
    "print(f'Sci-Kit Learn   V.{skl.__version__}')\n",
    "print(f'Seaborn         V.{sns.__version__}')\n",
    "print(f'TensorFlow      V.{str(tf.__version__)[:6]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "{}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 222\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \t\u001b[39m# Run for danger\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \tdanger_dict \u001b[39m=\u001b[39m Create_Score_Dictionary(\u001b[39m'\u001b[39m\u001b[39mdanger\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \tdanger_scores1 \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(danger_dict\u001b[39m.\u001b[39;49mvalues())[\u001b[39m1\u001b[39;49m] \u001b[39m# this is for the first criminal in our list of criminal_names\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \tdanger_names1 \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(danger_dict\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m]\n\u001b[1;32m    225\u001b[0m \t\u001b[39m# Now, let's use these lists in order to create a danger plot.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \t\u001b[39m# TODO: Insert function for plotting the score_list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \t\u001b[39m# get danger scores for the first criminal\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def Clean_Words(\n",
    "\t\tstring: str = None):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\tRemove formatting and punctuation from source.\n",
    "\n",
    "\tParameters:\n",
    "\t\tstring: str, String of input text.\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist[str]: String of tokenized text.\n",
    "\t\"\"\"\n",
    "\t# Define regular expressions to match words, punctuation, and sequences of dots\n",
    "\tword_regex = r'\\b\\w+\\b'\n",
    "\tpunct_regex = r'[^\\w\\s]'\n",
    "\t#dot_regex = r'\\.+'\n",
    "\tdot_regex = r'\\.{4,}|\\.{3}|\\.{2}|\\.'\n",
    "\t# Define a list to hold the resulting tokens\n",
    "\ttokens = []\n",
    "\t# Iterate over the matches of the regular expressions and append them to the tokens list\n",
    "\tfor match in re.finditer(f'{dot_regex}|{punct_regex}|{word_regex}', string):\n",
    "\t\ttoken = match.group()\n",
    "\t\tif re.match(word_regex, token):\n",
    "\t\t\ttokens.append([token])\n",
    "\t\telif re.match(dot_regex, token):\n",
    "\t\t\ttokens.append([token])\n",
    "\t\telse: # Split the token into parts that consist of either dots or non-dots\n",
    "\t\t\tparts = re.findall(f'(?:{dot_regex})+|[^\\.\\s]+', token)\n",
    "\t\t\ttokens.extend([part] for part in parts)\n",
    "\treturn [''.join(entry) for entry in tokens]\n",
    "\n",
    "\n",
    "def Get_Tokens_List(\n",
    "\t\tbook_name: str = None):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\tInitial extraction of strings from source.\n",
    "\n",
    "\tParameters:\n",
    "\t\tbook_name: str, File name and extension for source data.\n",
    "\n",
    "\tReturns:\n",
    "\t\tClean_Words(text_string): list[str], String of tokenized text.\n",
    "\t\"\"\"\n",
    "\tfile = open(book_name, 'r')\n",
    "\ttext = file.read()\n",
    "\t# replace all \\n with ' '\n",
    "\ttext = text.replace('\\n', ' ')\n",
    "\t# split everything into entries of a list\n",
    "\ttext = text.split(' ')\n",
    "\t# Remove all ' ' entries\n",
    "\ttext = [entry for entry in text if entry.strip()]\n",
    "\t# make everything lowercase\n",
    "\ttext = [entry.lower() for entry in text]\n",
    "\ttext_string = ' '.join(text)\n",
    "\t# Combine new_res elements into a string with spaces for separators\n",
    "\t# Get a list of all tokens by calling Clean_Words on text_string\n",
    "\treturn Clean_Words(text_string)\n",
    "\n",
    "def get_happiness_scores(\n",
    "\t\tngrams,\n",
    "\t\thedon,\n",
    "\t\thap_vars):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\tScore each of the ngrams based on a selection of emotional variables.\n",
    "\n",
    "\tParameters:\n",
    "\t\tngrams: str, Tokenized input text.\n",
    "\t\thedon: dict{str : float}, LabMT sentiment scores for english words.\n",
    "\t\thap_vars: list[list[str], list[float]],\n",
    "\t\t\t\t\t\t\t\tIndex 0: List of words as strings;\n",
    "\t\t\t\t\t\t\t\tIndex 1: List of scores ad floats.\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist[float], List of scores as floats.\n",
    "\t\"\"\"\n",
    "\t# hap_vars are hap_vars[0] = 'words' and hap_vars[1]='scores'\n",
    "\thap_df = hedon[hap_vars]\n",
    "\thap_mean = hap_df[hap_vars[1]].mean()\n",
    "\thap_dict = dict(zip(hap_df[hap_vars[0]], hap_df[hap_vars[1]]))\n",
    "\treturn [hap_dict[gram] if gram in list(hap_dict.keys()) else hap_mean for gram in ngrams]\n",
    "\n",
    "def Get_Token_Scores(\n",
    "\t\ttokens: str = None,\n",
    "\t\tdf_lexicon_scores: dict = None,\n",
    "\t\tscore_type: str = None,\n",
    "\t\temotion: str = 'danger'):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\tExtract danger scores for tokenized text.\n",
    "\n",
    "\tParameters:\n",
    "\t\ttokens: str, Tokenized input text.\n",
    "\t\tdf_lexicon_scores: dict{str : float}, LabMT scores for english words.\n",
    "\t\tscore_type: str, Desired metric to retrieve.\n",
    "\t\temotion: str, Default: 'danger'. See data source for alternate options.\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist[dict{str : float}], List of dictionary with word-score pairs.\n",
    "\t\"\"\"\n",
    "\t# create a dataframe from the tokens list\n",
    "\tdf_tokens = pd.DataFrame({'word': tokens})\n",
    "\n",
    "\t# merge the tokens dataframe with df_lexicon_scores\n",
    "\t# score_type can be power, danger, etc...\n",
    "\t# return the danger scores as a list\n",
    "\treturn pd.merge(\n",
    "\t\tdf_tokens,\n",
    "\t\tdf_lexicon_scores[['word', score_type]],\n",
    "\t\ton = 'word',\n",
    "\t\thow = 'left'\n",
    "\t)[emotion].tolist()\n",
    "\n",
    "def calculate_averages(\n",
    "\t\traw_scores,\n",
    "\t\twindow_size):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\t.\n",
    "\n",
    "\tParameters:\n",
    "\t\t.\n",
    "\n",
    "\tReturns:\n",
    "\t\t.\n",
    "\t\"\"\"\n",
    "\traw_series = pd.Series(raw_scores)\n",
    "\twindows = raw_series.rolling(window_size)\n",
    "\treturn windows.mean()\n",
    "\n",
    "def plotter(\n",
    "\t\tdata: list,\n",
    "\t\ttitles: str,\n",
    "\t\trows: int,\n",
    "\t\tcols: int):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\t.\n",
    "\n",
    "\tParameters:\n",
    "\t\t.\n",
    "\n",
    "\tReturns:\n",
    "\t\t.\n",
    "\t\"\"\"\n",
    "\tfig, axs = plt.subplots(\n",
    "\t\tnrows = rows,\n",
    "\t\tncols = cols,\n",
    "\t\tfigsize = (8, 10))\n",
    "\tfig.tight_layout()\n",
    "\tsns.despine(fig)\n",
    "\n",
    "\tfor avg, ax, title in zip(data, axs.ravel(), titles):\n",
    "\t\tax.plot(avg)\n",
    "\t\tax.set_title(title, fontsize=7)\n",
    "\t\tax.set_ylabel(r\"$h_{avg}$\", fontsize=7)\n",
    "\t\tax.tick_params(axis='both', which='minor', labelsize=5)\n",
    "\taxs.ravel()[-1].set_xlabel(\"Word number i\")\n",
    "\n",
    "def word_shifts(type2freq_1: dict, type2freq_2: dict, ref_avg: float, title: list):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "\t\t\t.\n",
    "\n",
    "    Parameters:\n",
    "\t\t\t.\n",
    "\n",
    "    Returns:\n",
    "\t\t\t.\n",
    "    \"\"\"\n",
    "    sentiment_shift = sh.WeightedAvgShift(\n",
    "\t    type2freq_1=type2freq_1,\n",
    "\t\t\ttype2freq_2=type2freq_2,\n",
    "\t\t\ttype2score_1='labMT_English',\n",
    "\t\t\treference_value=ref_avg,\n",
    "\t\t\tstop_lens=[(4,6)])\n",
    "\n",
    "    sentiment_shift.get_shift_graph(\n",
    "\t    detailed=True,\n",
    "\t\t\tsystem_names=[title[0],\n",
    "\t\t title[1]])\n",
    "\n",
    "    return sentiment_shift\n",
    "\n",
    "def Create_Score_Dictionary(\n",
    "\t\tscore_type):\n",
    "\t\"\"\"\n",
    "\tDescription:\n",
    "\t\t.\n",
    "\n",
    "\tParameters:\n",
    "\t\t.\n",
    "\n",
    "\tReturns:\n",
    "\t\t.\n",
    "\t\"\"\"\n",
    "\t# read in lexicon\n",
    "\tdf_lexicon_scores = pd.read_table('ousiometry_data_augmented.tsv')\n",
    "\t# create an empty dictionary to store the results\n",
    "\tresults_dict = {}\n",
    "\t# get a list of the .txt files in the 'shooters_words_text' directory\n",
    "\tfolder_path = 'shooters_words_text'\n",
    "\tfile_names = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\tprint(file_names)\n",
    "\t# loop over the file names and apply the function to each file\n",
    "\tfor file_name in file_names:\n",
    "\t\t# apply the function to the file contents\n",
    "\t\ttokens_list = Get_Tokens_List(f'{folder_path}/{file_name}')\n",
    "\t\ttokens_list = Get_Token_Scores(tokens_list, df_lexicon_scores, score_type)\n",
    "\t\t# store the output in the results dictionary with the file name as the key\n",
    "\t\tcriminal_name = file_name.split('.')[0]\n",
    "\t\tresults_dict[criminal_name] = tokens_list\n",
    "\t# print out and return the results dictionary\n",
    "\tprint(results_dict)\n",
    "\treturn results_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Run for danger\n",
    "\tdanger_dict = Create_Score_Dictionary('danger')\n",
    "\tdanger_scores1 = list(danger_dict.values())[1] # this is for the first criminal in our list of criminal_names\n",
    "\tdanger_names1 = list(danger_dict.values())[0]\n",
    "\n",
    "\t# Now, let's use these lists in order to create a danger plot.\n",
    "\n",
    "\t# TODO: Insert function for plotting the score_list\n",
    "\t# for name, score_list in danger_dict.items():\n",
    "\t#     ...\n",
    "\t# list(results_dict.values())\n",
    "\n",
    "\t# get danger scores for the first criminal\n",
    "\ta = 1.0\n",
    "\tb = 4.5\n",
    "\tc = 0.5\n",
    "\n",
    "\twindow_sizes = [\n",
    "\t\tround(10 ** i) for i in np.arange(1, 4.5, .5)\n",
    "\t\t]\n",
    "\n",
    "\trolling_averages = [\n",
    "\t\tcalculate_averages(danger_scores1,window_size) for window_size in window_sizes\n",
    "\t\t]\n",
    "\n",
    "\tt_list = [\n",
    "\t\tf\"\"\"\n",
    "\t\tInsert Criminal Name Here, T = {str(window_size)},\n",
    "\t\tz = {str(round(np.log10(window_size) * 2) / 2)}\n",
    "\t\t\"\"\"\n",
    "\t\tfor window_size in window_sizes\n",
    "\t]\n",
    "\n",
    "\tplotter(rolling_averages, t_list, 7, 1)\n",
    "\tplt.savefig(\"unadjusted_happiness.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
